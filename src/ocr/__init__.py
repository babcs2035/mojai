"""
OCR çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼

manga-ocr ã«ã‚ˆã‚‹è¡Œå˜ä½ã®èªè­˜çµæœã‚’åˆ©ç”¨ã—ï¼Œç”»åƒè§£æã«åŸºã¥ã„ãŸæ–‡å­—åˆ†å‰²ã‚’è¡Œãªã†ï¼
èªè­˜çµæœã¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç…§åˆã—ï¼Œå„æ–‡å­—ã®åº§æ¨™ç¯„å›²ã¨ç¢ºä¿¡åº¦ã‚’ç‰¹å®šã™ã‚‹ï¼
"""

import json
import webbrowser
from pathlib import Path

import numpy as np
from manga_ocr import MangaOcr
from PIL import Image
from scipy.ndimage import gaussian_filter1d
from scipy.signal import find_peaks

from src.config import settings
from src.ocr.preprocessor import Preprocessor
from src.ocr.report import ReportGenerator


class OCRPipeline:
    """
    OCR çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹ï¼

    ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã§å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ï¼š
    1. å‰å‡¦ç†ã«ã‚ˆã‚‹è¡Œé ˜åŸŸã®æ¤œå‡ºï¼
    2. manga-ocr ã«ã‚ˆã‚‹è¡Œå˜ä½ã®æ–‡å­—åˆ—èªè­˜ã¨ç¢ºä¿¡åº¦ã®å–å¾—ï¼
    3. èªè­˜çµæœã¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ç…§åˆï¼
    4. å‚ç›´æŠ•å½±ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨ã„ãŸå¹¾ä½•å­¦çš„ãªæ–‡å­—å¢ƒç•Œã®æ¨å®šï¼
    5. è§£æçµæœã‚’ã¾ã¨ã‚ãŸ HTML ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆï¼
    """

    def __init__(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–ã‚’è¡Œãªã†ï¼"""
        self.output_dir = settings.output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.preprocessor = Preprocessor()
        self.report_generator = ReportGenerator(self.output_dir)

        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        print("ğŸ§  Loading manga-ocr model...")
        self.mocr = MangaOcr()
        print("âœ… Model loaded successfully")

    def process(self) -> Path:
        """
        ç”»åƒå‡¦ç†ã‹ã‚‰ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã¾ã§ã®ä¸€é€£ã® OCR ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œã™ã‚‹ï¼

        Returns:
            Path: ç”Ÿæˆã•ã‚ŒãŸ HTML ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ‘ã‚¹ï¼
        """
        print("ğŸ“ Starting OCR process...")

        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹ã‚’ç‰¹å®š
        input_path = self._find_input_image()
        anno_path = settings.input_dir / "annotation.txt"

        if input_path is None:
            print(f"âŒ Error: Input image not found in {settings.input_dir}")
            return None

        if not anno_path.exists():
            print(f"âŒ Error: Annotation file not found: {anno_path}")
            return None

        # ç”»åƒãŠã‚ˆã³ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®èª­ã¿è¾¼ã¿
        image = Image.open(input_path).convert("RGB")
        with open(anno_path, encoding="utf-8") as f:
            anno_lines = [line.strip() for line in f if line.strip()]

        target_text = "".join(anno_lines)
        print(f"  ğŸ“Š Target: {len(target_text)} characters across {len(anno_lines)} lines")

        # 1. ç”»åƒã®å‰å‡¦ç†ï¼ˆäºŒå€¤åŒ–ãªã©ï¼‰
        _, binary = self.preprocessor.process(image)
        print("  âœ… Image preprocessing complete")

        # 2. è¡Œé ˜åŸŸã®æ¤œå‡º
        line_regions = self.preprocessor.detect_lines(binary)
        print(f"  ğŸ“ Detected {len(line_regions)} lines in the image")

        # è¡Œæ•°ã®ä¸ä¸€è‡´ã«é–¢ã™ã‚‹è­¦å‘Š
        if len(line_regions) != len(anno_lines):
            print(
                f"  âš ï¸ Warning: Line count mismatch! (Detected: {len(line_regions)}, Annotation: {len(anno_lines)})"
            )
            num_proc = min(len(line_regions), len(anno_lines))
        else:
            num_proc = len(line_regions)

        all_results = []
        char_idx = 0
        total_chars = 0
        line_matches = 0

        # 3. å„è¡Œã«å¯¾ã™ã‚‹å‡¦ç†ã®å®Ÿè¡Œ
        for line_no in range(num_proc):
            y1, y2 = line_regions[line_no]
            line_text = anno_lines[line_no]
            line_image = image.crop((0, y1, image.width, y2))
            line_binary = binary[y1:y2, :]

            print(f"  ğŸ“ Line {line_no + 1}: '{line_text[:15]}...' ({len(line_text)} chars)")

            # manga-ocr ã«ã‚ˆã‚‹èªè­˜ãŠã‚ˆã³ç¢ºä¿¡åº¦ã®ç®—å‡º
            recognized_text, char_confidences = self._recognize_with_confidence(line_image)
            print(f"    ğŸ” Recognized: '{recognized_text[:20]}...'")

            # è¡Œå˜ä½ã®èªè­˜ç²¾åº¦ãƒã‚§ãƒƒã‚¯
            if recognized_text == line_text:
                line_matches += 1
                print("    âœ… Perfect line match")
            else:
                print(f"    âš ï¸ Recognized {len(recognized_text)} chars (Target: {len(line_text)})")

            # æŠ•å½±è§£æã«ã‚ˆã‚‹æ–‡å­—å¢ƒç•Œã®æ¨å®š
            char_boundaries = self._find_character_boundaries(line_binary, len(line_text))

            # å„æ–‡å­—ã®æŠ½å‡ºã¨ä¿å­˜
            for i in range(len(line_text)):
                char = line_text[i]
                total_chars += 1

                # å¢ƒç•Œæƒ…å ±ã®å–å¾—ï¼ˆä¸è¶³æ™‚ã¯ç­‰åˆ†å‰²ã§è£œå®Œï¼‰
                if i < len(char_boundaries):
                    x_start, x_end = char_boundaries[i]
                else:
                    char_width = line_image.width / len(line_text)
                    x_start = int(i * char_width)
                    x_end = int((i + 1) * char_width)

                # åº§æ¨™è¨ˆç®—ã¨ä¿å­˜
                char_x, char_y = x_start, y1
                char_w, char_h = x_end - x_start, y2 - y1

                # ä½™è£•ã‚’æŒãŸã›ãŸã‚¯ãƒ­ãƒƒãƒ—ç¯„å›²ã®è¨­å®š
                margin = 3
                crop_x1 = max(0, char_x - margin)
                crop_y1 = max(0, char_y - margin)
                crop_x2 = min(image.width, char_x + char_w + margin)
                crop_y2 = min(image.height, char_y + char_h + margin)

                # æ–‡å­—ç”»åƒã®ä¿å­˜
                char_img = image.crop((crop_x1, crop_y1, crop_x2, crop_y2))
                char_filename = f"char_{char_idx:03d}_{char}.png"
                char_img.save(self.output_dir / char_filename)

                # èªè­˜çµæœã®ç´ä»˜ã‘ï¼ˆæ–‡å­—åˆ—é•·ãŒç•°ãªã‚‹å ´åˆã¯ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãŒã‚ºãƒ¬ã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼‰
                if i < len(recognized_text):
                    recognized_char = recognized_text[i]
                    conf = char_confidences[i] if i < len(char_confidences) else 0.0
                else:
                    recognized_char = "?"
                    conf = 0.0

                is_match = recognized_char == char

                # è§£æçµæœã®è“„ç©
                all_results.append(
                    {
                        "index": char_idx,
                        "text": char,
                        "recognized": recognized_char,
                        "verified": is_match,
                        "confidence": float(conf),
                        "bbox": [int(char_x), int(char_y), int(char_w), int(char_h)],
                        "image_path": char_filename,
                    }
                )
                char_idx += 1

        # çµ±è¨ˆæƒ…å ±ã®ç®—å‡º
        matched_chars = sum(1 for r in all_results if r["verified"])
        accuracy = matched_chars / total_chars if total_chars > 0 else 0
        print(f"  ğŸ¯ Overall accuracy: {accuracy:.1%} ({matched_chars}/{total_chars})")
        print(f"  ğŸ“Š Line match record: {line_matches}/{num_proc}")

        # 4. è§£æçµæœã® JSON å‡ºåŠ›
        final_output = {
            "source_path": str(input_path),
            "metadata": {
                "total_characters": len(all_results),
                "detector": "manga-ocr (Line-based) + Valley Detection",
                "accuracy": accuracy,
                "verified": matched_chars,
                "total": total_chars,
                "line_matches": line_matches,
            },
            "characters": all_results,
        }

        result_json_path = self.output_dir / "result.json"
        with open(result_json_path, "w", encoding="utf-8") as f:
            json.dump(final_output, f, ensure_ascii=False, indent=2)

        print("  ğŸ’¾ Results saved to JSON file")

        # 5. HTML ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã¨è¡¨ç¤º
        report_path = self.report_generator.generate(final_output)
        print(f"  ğŸ“„ Report generated at: {report_path}")

        webbrowser.open(f"file://{report_path.resolve()}")
        print("âœ… OCR process complete! Opening report...")

        return report_path

    def _recognize_with_confidence(self, image: Image.Image) -> tuple[str, list[float]]:
        """
        ç”»åƒã‚’ OCR ãƒ¢ãƒ‡ãƒ«ã«æŠ•å…¥ã—ï¼Œå„ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºä¿¡åº¦ã‚’å–å¾—ã™ã‚‹ï¼

        Args:
            image (Image.Image): èªè­˜å¯¾è±¡ã®è¡Œç”»åƒï¼

        Returns:
            tuple[str, list[float]]: èªè­˜ã•ã‚ŒãŸæ–‡å­—åˆ—ã¨ï¼Œå„æ–‡å­—ã«å¯¾å¿œã™ã‚‹ç¢ºä¿¡åº¦ã®ãƒªã‚¹ãƒˆï¼
        """
        import torch
        from transformers import ViTImageProcessor

        # ãƒ¢ãƒ‡ãƒ«å†…éƒ¨ã§ä½¿ç”¨ã•ã‚Œã‚‹ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’å–å¾—
        processor = ViTImageProcessor.from_pretrained("kha-white/manga-ocr-base")

        pixel_values = processor(image, return_tensors="pt").pixel_values
        pixel_values = pixel_values.to(self.mocr.model.device)

        # ç”Ÿæˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆã‚¹ã‚³ã‚¢å‡ºåŠ›ã‚’æœ‰åŠ¹åŒ–ï¼‰
        with torch.no_grad():
            outputs = self.mocr.model.generate(
                pixel_values,
                output_scores=True,
                return_dict_in_generate=True,
                max_length=300,
            )

        # 1. èªè­˜æ–‡å­—åˆ—ã®å–å¾—
        sequences = outputs.sequences
        decoded_text = self.mocr.tokenizer.batch_decode(sequences, skip_special_tokens=True)[0]
        # Transformers ãŒæŒ¿å…¥ã™ã‚‹ä½™åˆ†ãªç©ºç™½ã‚’é™¤å»
        decoded_text = decoded_text.replace(" ", "")

        # 2. å„ç”Ÿæˆã‚¹ãƒ†ãƒƒãƒ—ã«ãŠã‘ã‚‹ç¢ºç‡ï¼ˆç¢ºä¿¡åº¦ï¼‰ã®ç®—å‡º
        scores = outputs.scores  # å„ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ­ã‚¸ãƒƒãƒˆï¼ˆtupleï¼‰
        token_confidences = []

        for i, score_tensor in enumerate(scores):
            # å®Ÿéš›ã«å‡ºåŠ›ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºç‡å€¤ã‚’è¨ˆç®—
            token_id = sequences[0][i + 1]  # sequences[0][0] ã¯ [CLS] ãƒˆãƒ¼ã‚¯ãƒ³

            # çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ°é”ã—ãŸå ´åˆã¯åœæ­¢
            if token_id == self.mocr.tokenizer.sep_token_id:
                break

            probs = torch.softmax(score_tensor, dim=-1)
            prob = probs[0, token_id].item()
            token_confidences.append(prob)

        # 3. æ–‡å­—ã¨ç¢ºä¿¡åº¦ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆèª¿æ•´
        # ç°¡æ˜“çš„ãªå®Ÿè£…ã¨ã—ã¦ï¼Œãƒ‡ã‚³ãƒ¼ãƒ‰çµæœã®æ–‡å­—é•·ã«åˆã‚ã›ã¦ãƒªã‚¹ãƒˆã‚’ãƒªã‚µã‚¤ã‚ºã™ã‚‹
        char_confidences = []
        if len(decoded_text) == len(token_confidences):
            char_confidences = token_confidences
        else:
            avg_conf = sum(token_confidences) / len(token_confidences) if token_confidences else 0.0
            char_confidences = token_confidences[: len(decoded_text)]
            while len(char_confidences) < len(decoded_text):
                char_confidences.append(avg_conf)

        return decoded_text, char_confidences

    def _find_character_boundaries(
        self, line_binary: np.ndarray, num_chars: int
    ) -> list[tuple[int, int]]:
        """
        è¡Œç”»åƒã«å¯¾ã—ã¦å‚ç›´æŠ•å½±è§£æã‚’è¡Œãªã„ï¼Œæ–‡å­—ã®å¢ƒç•Œã‚’æ¨å®šã™ã‚‹ï¼ˆè°·ç‚¹æ¤œå‡ºï¼‰ï¼

        Args:
            line_binary (np.ndarray): äºŒå€¤åŒ–ã•ã‚ŒãŸè¡Œç”»åƒï¼
            num_chars (int): ãã®è¡Œã«å«ã¾ã‚Œã‚‹ã¹ãæ–‡å­—æ•°ï¼

        Returns:
            list[tuple[int, int]]: å„æ–‡å­—ã®å·¦å³ã®å¢ƒç•Œåº§æ¨™ï¼ˆx_start, x_endï¼‰ã®ãƒªã‚¹ãƒˆï¼
        """
        height, width = line_binary.shape

        # å‚ç›´æ–¹å‘ã¸ã®æŠ•å½±ï¼ˆã‚¤ãƒ³ã‚¯é‡ã®ç©ç®—ï¼‰
        projection = np.sum(line_binary, axis=0).astype(float)
        # ä¿¡å·ã®å¹³æ»‘åŒ–
        smoothed = gaussian_filter1d(projection, sigma=3)

        # ã‚¤ãƒ³ã‚¯ãŒå­˜åœ¨ã™ã‚‹æœ‰åŠ¹é ˜åŸŸã‚’ç‰¹å®š
        ink_threshold = np.max(smoothed) * 0.02
        ink_mask = smoothed > ink_threshold

        ink_start = 0
        ink_end = width
        for i in range(width):
            if ink_mask[i]:
                ink_start = max(0, i - 3)
                break
        for i in range(width - 1, -1, -1):
            if ink_mask[i]:
                ink_end = min(width, i + 3)
                break

        # ã‚¤ãƒ³ã‚¯ãŒå…¨ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç­‰å¹…åˆ†å‰²ï¼‰
        if ink_end <= ink_start:
            char_width = width / num_chars
            return [(int(i * char_width), int((i + 1) * char_width)) for i in range(num_chars)]

        ink_region = smoothed[ink_start:ink_end]
        estimated_char_width = len(ink_region) / num_chars
        min_distance = max(5, int(estimated_char_width * 0.3))

        # ã‚¤ãƒ³ã‚¯é‡ãŒæ¥µå°ã¨ãªã‚‹ç®‡æ‰€ï¼ˆè°·ç‚¹ï¼‰ã‚’æŠ½å‡º
        valleys, _ = find_peaks(
            -ink_region, distance=min_distance, prominence=np.max(ink_region) * 0.05
        )
        valleys = valleys + ink_start

        # å¿…è¦ãªåŒºåˆ‡ã‚Šä½ç½®ã®æ•°
        needed = num_chars - 1

        if len(valleys) >= needed:
            # è°·ã®æ·±ã•ï¼ˆã‚¤ãƒ³ã‚¯é‡ã®å°‘ãªã•ï¼‰ã«åŸºã¥ã„ã¦ä¸Šä½ã‚’æ¡ç”¨
            depths = smoothed[valleys]
            sorted_indices = np.argsort(depths)[:needed]
            boundaries = sorted(valleys[sorted_indices])
        elif len(valleys) > 0:
            # è°·ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆï¼Œåºƒã„é ˜åŸŸã®ä¸­å¤®ã‚’è¿½åŠ ã—ã¦è£œå®Œ
            boundaries = list(valleys)
            all_points = [ink_start] + list(boundaries) + [ink_end]
            while len(boundaries) < needed:
                gaps = [(all_points[i + 1] - all_points[i], i) for i in range(len(all_points) - 1)]
                gaps.sort(reverse=True)
                widest_idx = gaps[0][1]
                mid = (all_points[widest_idx] + all_points[widest_idx + 1]) // 2
                boundaries.append(mid)
                boundaries.sort()
                all_points = [ink_start] + list(boundaries) + [ink_end]
        else:
            # è°·ãŒå…¨ãæ¤œå‡ºã•ã‚Œãªã„å ´åˆã¯å˜ã«å‡ç­‰åˆ†å‰²
            ink_width = ink_end - ink_start
            boundaries = [ink_start + int(ink_width * (i + 1) / num_chars) for i in range(needed)]

        # å¢ƒç•Œåº§æ¨™ã®ãƒªã‚¹ãƒˆã‚’æ§‹æˆ
        all_bounds = [ink_start] + list(boundaries) + [ink_end]
        return [(all_bounds[i], all_bounds[i + 1]) for i in range(num_chars)]

    def _find_input_image(self) -> Path | None:
        """ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ã™ã‚‹ï¼"""
        for ext in ["png", "jpg", "jpeg", "PNG", "JPG", "JPEG"]:
            path = settings.input_dir / f"image.{ext}"
            if path.exists():
                return path
        return None
