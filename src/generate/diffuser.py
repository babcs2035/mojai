"""
FontDiffuser ãƒ©ãƒƒãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼

æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ï¼Œä¸€æ–‡å­—ã®å‚ç…§ç”»åƒã‹ã‚‰ä»»æ„ã®æ–‡å­—ç¨®ã®ãƒ•ã‚©ãƒ³ãƒˆç”»åƒã‚’ç”Ÿæˆã™ã‚‹ï¼
RTX 3090ï¼ˆVRAM 24GBï¼‰ã®è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã«æœ€é©åŒ–ã•ã‚Œã¦ãŠã‚Šï¼Œ
é«˜åº¦ãªã‚¹ã‚¿ã‚¤ãƒ«è»¢å†™ã¨æ§‹é€ ç¶­æŒã‚’ä¸¡ç«‹ã™ã‚‹ï¼
"""

from dataclasses import dataclass
from pathlib import Path
from typing import Any

import numpy as np
import torch
from PIL import Image, ImageDraw, ImageFont

from src.config import settings


@dataclass
class GenerationConfig:
    """ãƒ•ã‚©ãƒ³ãƒˆç”Ÿæˆã«é–¢ã™ã‚‹è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿æŒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ï¼"""

    num_inference_steps: int = 25  # æ¨è«–ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°
    guidance_scale: float = 7.5  # åˆ†é¡å™¨ãƒ•ãƒªãƒ¼ãƒ»ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã®å°ºåº¦
    strength: float = 0.75  # img2img ã«ãŠã‘ã‚‹å…ƒç”»åƒã®ä¿æŒå¼·åº¦
    batch_size: int = 32  # GPU ã§ã®ä¸€æ‹¬å‡¦ç†ã‚µã‚¤ã‚º
    use_fp16: bool = True  # åŠç²¾åº¦æµ®å‹•å°æ•°ç‚¹ï¼ˆFP16ï¼‰ã«ã‚ˆã‚‹é«˜é€ŸåŒ–ã®æœ‰ç„¡
    seed: int | None = None  # å†ç¾æ€§ã®ãŸã‚ã®ä¹±æ•°ã‚·ãƒ¼ãƒ‰
    image_size: int = 64  # å‡ºåŠ›ç”»åƒã®è§£åƒåº¦


class ContentRenderer:
    """
    æ–‡å­—ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆéª¨æ ¼ï¼‰ã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚’æ‹…å½“ã™ã‚‹ã‚¯ãƒ©ã‚¹ï¼

    æ¨™æº–ãƒ•ã‚©ãƒ³ãƒˆï¼ˆNoto Sans CJK JP ç­‰ï¼‰ã‚’ç”¨ã„ã¦ï¼Œç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«æŠ•å…¥ã™ã‚‹å…¥åŠ›ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ï¼
    """

    def __init__(self, font_path: Path | str | None = None, font_size: int = 48):
        """
        ãƒ¬ãƒ³ãƒ€ãƒ©ãƒ¼ã‚’åˆæœŸåŒ–ã™ã‚‹ï¼

        Args:
            font_path (Path | str, optional): å‚ç…§ã«ä½¿ç”¨ã™ã‚‹ãƒ•ã‚©ãƒ³ãƒˆã®ãƒ‘ã‚¹ï¼
            font_size (int): ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°æ™‚ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºï¼
        """
        self.font_size = font_size
        self._font: ImageFont.FreeTypeFont | None = None

        if font_path:
            self.font_path = Path(font_path)
        else:
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ•ã‚©ãƒ³ãƒˆã‚’æ¤œç´¢
            default_font = settings.models_dir / "fonts" / "NotoSansCJKjp-Regular.otf"
            self.font_path = default_font if default_font.exists() else None

    def _load_font(self) -> ImageFont.FreeTypeFont | ImageFont.ImageFont:
        """ãƒ•ã‚©ãƒ³ãƒˆãƒªã‚½ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã‚€ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰ï¼"""
        if self._font is not None:
            return self._font

        if self.font_path and self.font_path.exists():
            self._font = ImageFont.truetype(str(self.font_path), self.font_size)
        else:
            # ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ä»£ç”¨
            self._font = ImageFont.load_default()

        return self._font

    def render(self, character: str, size: int = 64) -> Image.Image:
        """
        æŒ‡å®šã•ã‚ŒãŸæ–‡å­—ã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã—ï¼Œã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒã‚’è¿”ã™ï¼

        Args:
            character (str): å¯¾è±¡æ–‡å­—ï¼
            size (int): ã‚­ãƒ£ãƒ³ãƒã‚¹ã‚µã‚¤ã‚ºï¼

        Returns:
            Image.Image: ç™½èƒŒæ™¯ã«é»’æ–‡å­—ã§æç”»ã•ã‚ŒãŸç”»åƒï¼
        """
        font = self._load_font()

        # ç™½èƒŒæ™¯ã®ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒã‚’ä½œæˆ
        img = Image.new("L", (size, size), color=255)
        draw = ImageDraw.Draw(img)

        # ãƒ†ã‚­ã‚¹ãƒˆã®æç”»ç¯„å›²ã‚’è¨ˆç®—ã—ï¼Œä¸­å¤®ã«é…ç½®ã™ã‚‹
        bbox = draw.textbbox((0, 0), character, font=font)
        text_width = bbox[2] - bbox[0]
        text_height = bbox[3] - bbox[1]

        x = (size - text_width) // 2 - bbox[0]
        y = (size - text_height) // 2 - bbox[1]

        draw.text((x, y), character, font=font, fill=0)

        return img

    def render_batch(self, characters: list[str], size: int = 64) -> list[Image.Image]:
        """è¤‡æ•°ã®æ–‡å­—ã‚’ä¸€æ‹¬ã§ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã™ã‚‹ï¼"""
        return [self.render(char, size) for char in characters]


class FontDiffuserWrapper:
    """
    FontDiffuser ãƒ¢ãƒ‡ãƒ«ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹ï¼

    Diffusers ãƒ©ã‚¤ãƒ–ãƒªã‚’æ´»ç”¨ã—ï¼ŒStable Diffusion ãƒ™ãƒ¼ã‚¹ã®ã‚¹ã‚¿ã‚¤ãƒ«è»¢å†™ã‚’å®Ÿç¾ã™ã‚‹ï¼
    GPU ãƒªã‚½ãƒ¼ã‚¹ãŒåˆ©ç”¨ä¸å¯èƒ½ãªå ´åˆã¯ï¼Œè‡ªå‹•çš„ã«å¤å…¸çš„ãªç”»åƒå‡¦ç†ï¼ˆOpenCVï¼‰ã«ã‚ˆã‚‹
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’è¡Œãªã†ï¼
    """

    def __init__(
        self,
        model_path: Path | str | None = None,
        device: str | None = None,
        config: GenerationConfig | None = None,
    ):
        """
        ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–ã™ã‚‹ï¼

        Args:
            model_path (Path | str, optional): ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ‘ã‚¹ï¼
            device (str, optional): æ¼”ç®—ãƒ‡ãƒã‚¤ã‚¹ï¼ˆ"cuda" ã¾ãŸã¯ "cpu"ï¼‰ï¼
            config (GenerationConfig, optional): ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šï¼
        """
        self.model_path = Path(model_path) if model_path else settings.diffuser_model_path
        self.device = device or self._detect_device()
        self.config = config or GenerationConfig(
            use_fp16=settings.diffuser_use_fp16,
            batch_size=settings.diffuser_batch_size,
            num_inference_steps=settings.diffuser_num_inference_steps,
        )

        self._pipeline: Any = None
        self._content_renderer = ContentRenderer()
        self._is_loaded = False

    def _detect_device(self) -> str:
        """ã‚·ã‚¹ãƒ†ãƒ ä¸Šã®æœ€é©ãªãƒ‡ãƒã‚¤ã‚¹ï¼ˆCUDA ã¾ãŸã¯ CPUï¼‰ã‚’ç‰¹å®šã™ã‚‹ï¼"""
        if torch.cuda.is_available():
            return f"cuda:{settings.cuda_device}"
        return "cpu"

    def _load_model(self) -> None:
        """ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ï¼ˆåˆå›å®Ÿè¡Œæ™‚ã®ã¿å®Ÿè¡Œã•ã‚Œã‚‹é…å»¶ãƒ­ãƒ¼ãƒ‰ï¼‰ï¼"""
        if self._is_loaded:
            return

        try:
            # å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ãŠã‚ˆã³ãƒ­ãƒ¼ãƒ‰
            if self.model_path and self.model_path.exists():
                print(f"ğŸ“¦ Loading custom FontDiffuser model from {self.model_path}...")
                self._load_custom_model()
            else:
                print("ğŸ“¦ Loading baseline stable-diffusion-v1-5 for font generation...")
                self._load_sd_pipeline()

            self._is_loaded = True

        except Exception as e:
            print(f"âš ï¸ Failed to load diffusion model: {e}")
            print("ğŸ’¡ Falling back to classical image processing engine")
            self._is_loaded = True

    def _load_custom_model(self) -> None:
        """ç‰¹å®šã®ãƒ‘ã‚¹ã‹ã‚‰ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’æ¸ˆã¿ã® FontDiffuser ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ï¼"""
        from diffusers import StableDiffusionImg2ImgPipeline

        self._pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(
            str(self.model_path),
            torch_dtype=torch.float16 if self.config.use_fp16 else torch.float32,
            safety_checker=None,
        )
        self._pipeline.to(self.device)

        if "cuda" in self.device:
            self._pipeline.enable_attention_slicing()

    def _load_sd_pipeline(self) -> None:
        """æ¨™æº–çš„ãª Stable Diffusion img2img ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ï¼"""
        from diffusers import StableDiffusionImg2ImgPipeline

        self._pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float16 if self.config.use_fp16 else torch.float32,
            safety_checker=None,
            variant="fp16" if self.config.use_fp16 else None,
        )
        self._pipeline.to(self.device)

        if "cuda" in self.device:
            self._pipeline.enable_attention_slicing()

    def extract_style(self, style_image: Image.Image | np.ndarray | Path | str) -> dict:
        """
        æ‰‹æ›¸ãã‚¹ã‚¿ã‚¤ãƒ«å‚ç…§ç”»åƒã‹ã‚‰è¦–è¦šçš„ç‰¹å¾´ï¼ˆã‚¹ã‚¿ã‚¤ãƒ«æƒ…å ±ï¼‰ã‚’æŠ½å‡ºã™ã‚‹ï¼

        Args:
            style_image: ä¸€æ–‡å­—åˆ†ã®å‚ç…§ç”»åƒï¼

        Returns:
            dict: ç”»åƒã®çµ±è¨ˆæƒ…å ±ã‚„æ­£è¦åŒ–ã•ã‚ŒãŸç”»åƒã‚’å«ã‚€ã‚¹ã‚¿ã‚¤ãƒ«è¾æ›¸ï¼
        """
        # ç”»åƒå½¢å¼ã®çµ±ä¸€
        if isinstance(style_image, (Path, str)):
            img = Image.open(style_image)
        elif isinstance(style_image, np.ndarray):
            img = Image.fromarray(style_image)
        else:
            img = style_image.copy()

        if img.mode != "L":
            img = img.convert("L")

        # æŒ‡å®šã•ã‚ŒãŸè§£åƒåº¦ã«ãƒªã‚µã‚¤ã‚º
        img = img.resize(
            (self.config.image_size, self.config.image_size),
            Image.Resampling.LANCZOS,
        )

        img_array = np.array(img, dtype=np.float32) / 255.0

        # åŸºæœ¬çš„ãªçµ±è¨ˆæƒ…å ±ã®ç®—å‡º
        style_info = {
            "image": img,
            "mean_intensity": float(np.mean(img_array)),
            "std_intensity": float(np.std(img_array)),
            "stroke_width": self._estimate_stroke_width(img_array),
        }

        return style_info

    def _estimate_stroke_width(self, img_array: np.ndarray) -> float:
        """
        è·é›¢å¤‰æ›ã‚’ç”¨ã„ã¦æ–‡å­—ã®ç·šã®å¤ªã•ï¼ˆã‚¹ãƒˆãƒ­ãƒ¼ã‚¯å¹…ï¼‰ã‚’æ¨å®šã™ã‚‹ï¼

        å¤å…¸çš„ç”»åƒå‡¦ç†ã§ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã«ï¼Œã‚°ãƒªãƒ•ã®å¤ªã•ã‚’åˆã‚ã›ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ï¼
        """
        import cv2

        # äºŒå€¤åŒ–ã¨è·é›¢å¤‰æ›
        binary = (img_array < 0.5).astype(np.uint8) * 255
        dist = cv2.distanceTransform(binary, cv2.DIST_L2, 5)

        if dist.max() > 0:
            return float(dist.max() * 2)
        return 2.0

    def generate(
        self,
        style_info: dict,
        target_characters: list[str],
        content_font: str | None = None,
    ) -> list[Image.Image]:
        """
        æŠ½å‡ºã•ã‚ŒãŸã‚¹ã‚¿ã‚¤ãƒ«æƒ…å ±ã‚’å…ƒã«ï¼Œç‰¹å®šã®æ–‡å­—ãƒªã‚¹ãƒˆã«å¯¾ã™ã‚‹ã‚°ãƒªãƒ•ã‚’ç”Ÿæˆã™ã‚‹ï¼

        Args:
            style_info (dict): æŠ½å‡ºæ¸ˆã¿ã®ã‚¹ã‚¿ã‚¤ãƒ«ç‰¹å¾´é‡ï¼
            target_characters (list[str]): ç”Ÿæˆå¯¾è±¡ã®æ–‡å­—ãƒªã‚¹ãƒˆï¼
            content_font (str, optional): ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ãƒ•ã‚©ãƒ³ãƒˆåï¼

        Returns:
            list[Image.Image]: ç”Ÿæˆã•ã‚ŒãŸç”»åƒãƒªã‚¹ãƒˆï¼
        """
        self._load_model()

        if content_font:
            self._content_renderer = ContentRenderer(content_font)

        generated_images: list[Image.Image] = []
        style_image = style_info["image"]

        # ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã®è¨­å®š
        generator = None
        if self.config.seed is not None:
            generator = torch.Generator(device=self.device).manual_seed(self.config.seed)

        # ãƒãƒƒãƒå˜ä½ã§ã®ç”Ÿæˆå®Ÿè¡Œ
        for i in range(0, len(target_characters), self.config.batch_size):
            batch_chars = target_characters[i : i + self.config.batch_size]
            batch_images = self._generate_batch(batch_chars, style_image, style_info, generator)
            generated_images.extend(batch_images)

        return generated_images

    def _generate_batch(
        self,
        characters: list[str],
        style_image: Image.Image,
        style_info: dict,
        generator: torch.Generator | None,
    ) -> list[Image.Image]:
        """æŒ‡å®šã•ã‚ŒãŸãƒãƒƒãƒå†…ã®æ–‡å­—ç”»åƒã‚’ä¸€æ‹¬ç”Ÿæˆã™ã‚‹ï¼"""
        generated = []
        for char in characters:
            # éª¨æ ¼ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼‰ç”»åƒã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
            content_image = self._content_renderer.render(char, self.config.image_size)

            # ã‚¹ã‚¿ã‚¤ãƒ«è»¢å†™ã®å®Ÿè¡Œï¼ˆæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯å¤å…¸çš„æ‰‹æ³•ï¼‰
            styled_image = self._apply_style(content_image, style_image, style_info, generator)
            generated.append(styled_image)

        return generated

    def _apply_style(
        self,
        content_image: Image.Image,
        style_image: Image.Image,
        style_info: dict,
        generator: torch.Generator | None,
    ) -> Image.Image:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”»åƒã«å¯¾ã—ã¦ã‚¹ã‚¿ã‚¤ãƒ«ã‚’è»¢å†™ã™ã‚‹ã‚¨ãƒ³ãƒˆãƒªãƒ¡ã‚½ãƒƒãƒ‰ï¼"""
        if self._pipeline is not None:
            return self._apply_style_diffusion(content_image, style_image, style_info, generator)
        else:
            return self._apply_style_classical(content_image, style_image, style_info)

    def _apply_style_diffusion(
        self,
        content_image: Image.Image,
        style_image: Image.Image,
        style_info: dict,
        generator: torch.Generator | None,
    ) -> Image.Image:
        """
        æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ï¼ˆimg2imgï¼‰ã‚’ç”¨ã„ãŸã‚¹ã‚¿ã‚¤ãƒ«è»¢å†™ï¼

        æ¨™æº–ãƒ•ã‚©ãƒ³ãƒˆã«ã‚ˆã‚‹éª¨æ ¼ã‚’ã‚¬ã‚¤ãƒ‰ã¨ã—ã¦ï¼Œæ‰‹æ›¸ãé¢¨ã®ãƒ†ã‚¯ã‚¹ãƒãƒ£ã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ï¼
        """
        # SD ã¯ RGB å…¥åŠ›ã‚’å‰æã¨ã™ã‚‹ãŸã‚å¤‰æ›
        content_rgb = content_image.convert("RGB")

        # ç”Ÿæˆã‚’èª˜å°ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹æˆ
        prompt = "handwritten Japanese character, calligraphy style, black ink on white paper"
        negative_prompt = "blurry, low quality, distorted, extra strokes"

        try:
            result = self._pipeline(
                prompt=prompt,
                negative_prompt=negative_prompt,
                image=content_rgb,
                strength=self.config.strength,
                num_inference_steps=self.config.num_inference_steps,
                guidance_scale=self.config.guidance_scale,
                generator=generator,
            )

            # å‡ºåŠ›çµæœã‚’ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã¨ã—ã¦å‡¦ç†ã—ï¼Œã‚µã‚¤ã‚ºã‚’æ­£è¦åŒ–
            generated = result.images[0].convert("L")
            return generated.resize(
                (self.config.image_size, self.config.image_size),
                Image.Resampling.LANCZOS,
            )

        except Exception:
            # æ¨è«–å¤±æ•—æ™‚ã¯å¤å…¸çš„æ‰‹æ³•ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._apply_style_classical(content_image, style_image, style_info)

    def _apply_style_classical(
        self,
        content_image: Image.Image,
        style_image: Image.Image,
        style_info: dict,
    ) -> Image.Image:
        """
        OpenCV ç­‰ã‚’ç”¨ã„ãŸå¤å…¸çš„ãªç”»åƒå‡¦ç†ã«ã‚ˆã‚‹ã‚¹ã‚¿ã‚¤ãƒ«è»¢å†™ï¼

        ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ç’°å¢ƒã‚„ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ä¸‹ã§ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦æ©Ÿèƒ½ã—ï¼Œ
        ç·šå¹…ã®èª¿æ•´ã¨ãƒã‚¤ã‚ºä»˜ä¸ã«ã‚ˆã‚Šæ‰‹æ›¸ãã‚‰ã—ã•ã‚’å†ç¾ã™ã‚‹ï¼
        """
        import cv2

        content_arr = np.array(content_image, dtype=np.float32)

        # äºŒå€¤åŒ–å‡¦ç†
        _, content_binary = cv2.threshold(
            content_arr.astype(np.uint8), 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU
        )

        # æ¨å®šã•ã‚ŒãŸç·šå¹…ã«åŸºã¥ã„ãŸè†¨å¼µãƒ»åç¸®
        stroke_width = style_info.get("stroke_width", 2.0)
        kernel_size = max(1, int(stroke_width / 2))
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))

        if stroke_width > 2:
            styled = cv2.dilate(content_binary, kernel, iterations=1)
        else:
            styled = content_binary

        # æ‰‹æ›¸ãæ„Ÿã‚’æ¼”å‡ºã™ã‚‹ãŸã‚ã®å¾®ç´°ãªãƒã‚¤ã‚ºã¨ã‚¬ã‚¦ã‚¹ã¼ã‹ã—ã®é©ç”¨
        noise = np.random.normal(0, 5, styled.shape).astype(np.float32)
        styled = np.clip(styled.astype(np.float32) + noise, 0, 255).astype(np.uint8)
        styled = cv2.GaussianBlur(styled, (3, 3), 0.5)

        # ç™½èƒŒæ™¯ãƒ»é»’æ–‡å­—ã¸ã®å¤‰æ›
        styled = 255 - styled

        return Image.fromarray(styled)

    def release(self) -> None:
        """GPU ãƒ¡ãƒ¢ãƒªãŠã‚ˆã³ãƒ¢ãƒ‡ãƒ«ãƒªã‚½ãƒ¼ã‚¹ã‚’å®‰å…¨ã«è§£æ”¾ã—ï¼Œæ¬¡ã®ã‚¿ã‚¹ã‚¯ã¸ã®ãƒªã‚½ãƒ¼ã‚¹ç«¶åˆã‚’é˜²ãï¼"""
        if self._pipeline is not None:
            del self._pipeline
            self._pipeline = None

        self._is_loaded = False

        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("ğŸ§¹ GPU resources released")
